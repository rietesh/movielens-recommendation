{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4. , 0. , 4. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [2.5, 2. , 2. , ..., 0. , 0. , 0. ],\n",
       "       [3. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [5. , 0. , 0. , ..., 0. , 0. , 0. ]], shape=(610, 9724))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Load MovieLens dataset\n",
    "df = pd.read_csv(\"./ml-latest-small/ratings.csv\")\n",
    "df['cat_movie'] = df['movieId'].astype(\"category\").cat.codes\n",
    "\n",
    "num_users = df['userId'].nunique()\n",
    "num_items = df['movieId'].nunique()\n",
    "\n",
    "# Create user-item rating matrix\n",
    "user_item_matrix = df.pivot(index='userId', columns='cat_movie', values='rating').fillna(0)\n",
    "\n",
    "# Convert to NumPy array\n",
    "ratings_matrix = user_item_matrix.to_numpy()\n",
    "ratings_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.18187197e+00,  3.93674189e-01,  8.38185756e-01, ...,\n",
       "        -2.49842711e-02, -2.49842711e-02, -5.89881001e-02],\n",
       "       [ 2.09809067e-01,  4.82051887e-03,  3.07424005e-02, ...,\n",
       "         1.88951263e-02,  1.88951263e-02,  3.19658766e-02],\n",
       "       [ 1.33940814e-02,  3.47258164e-02,  5.05247472e-02, ...,\n",
       "        -1.61232411e-03, -1.61232411e-03, -5.29984436e-04],\n",
       "       ...,\n",
       "       [ 2.30963539e+00,  2.70243898e+00,  2.26419696e+00, ...,\n",
       "        -1.25165145e-02, -1.25165145e-02,  9.27520866e-02],\n",
       "       [ 7.83182598e-01,  5.30142683e-01,  9.79748203e-02, ...,\n",
       "         9.84577917e-04,  9.84577917e-04, -5.49383653e-03],\n",
       "       [ 5.35809290e+00, -2.88817350e-01, -9.07680249e-02, ...,\n",
       "        -2.79227416e-02, -2.79227416e-02,  3.55476113e-02]],\n",
       "      shape=(610, 9724))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute SVD (k=50 latent features)\n",
    "k = 50\n",
    "U, sigma, Vt = svds(ratings_matrix, k=k)\n",
    "\n",
    "# Convert singular values to diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Reconstruct approximate ratings matrix\n",
    "predicted_ratings = np.dot(np.dot(U, sigma), Vt)\n",
    "predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movies for user 10: [4131 4791 3633  314 4354]\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_id, predicted_ratings, num_recommendations=5):\n",
    "    user_row = user_id - 1  # Adjust for zero-based indexing\n",
    "    sorted_movies = np.argsort(predicted_ratings[user_row])[::-1]  # Sort by predicted rating\n",
    "    return sorted_movies[:num_recommendations]\n",
    "\n",
    "# Example recommendation\n",
    "user_id = 10\n",
    "recommended_movies = recommend_for_user(user_id, predicted_ratings)\n",
    "print(f\"Recommended movies for user {user_id}: {recommended_movies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF Matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4. , 0. , 4. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [2.5, 2. , 2. , ..., 0. , 0. , 0. ],\n",
       "       [3. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [5. , 0. , 0. , ..., 0. , 0. , 0. ]], shape=(610, 9724))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Load MovieLens dataset\n",
    "df = pd.read_csv(\"./ml-latest-small/ratings.csv\")\n",
    "df['cat_movie'] = df['movieId'].astype(\"category\").cat.codes\n",
    "\n",
    "num_users = df['userId'].nunique()\n",
    "num_items = df['movieId'].nunique()\n",
    "\n",
    "# Create user-item rating matrix\n",
    "user_item_matrix = df.pivot(index='userId', columns='cat_movie', values='rating').fillna(0)\n",
    "\n",
    "# Convert to NumPy array\n",
    "ratings_matrix = user_item_matrix.to_numpy()\n",
    "ratings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NMF (k=50 latent features)\n",
    "k = 50\n",
    "nmf_model = NMF(n_components=k, init='random', random_state=42, max_iter=500)\n",
    "W = nmf_model.fit_transform(ratings_matrix)\n",
    "H = nmf_model.components_\n",
    "\n",
    "# Reconstruct predicted ratings\n",
    "predicted_ratings = np.dot(W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movies for user 10: [6693  277 7355  314 4791]\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_id, predicted_ratings, num_recommendations=5):\n",
    "    user_row = user_id - 1  # Adjust for zero-based indexing\n",
    "    sorted_movies = np.argsort(predicted_ratings[user_row])[::-1]  # Sort by predicted rating\n",
    "    return sorted_movies[:num_recommendations]\n",
    "\n",
    "# Example recommendation\n",
    "user_id = 10\n",
    "recommended_movies = recommend_for_user(user_id, predicted_ratings)\n",
    "print(f\"Recommended movies for user {user_id}: {recommended_movies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load MovieLens dataset\n",
    "df = pd.read_csv(\"./ml-latest-small/ratings.csv\")\n",
    "num_users = df['userId'].nunique()\n",
    "num_items = df['movieId'].nunique()\n",
    "df['cat_movie'] = df['movieId'].astype(\"category\").cat.codes\n",
    "\n",
    "# Convert user-item interactions to a matrix\n",
    "user_item_matrix = np.zeros((num_users, num_items))\n",
    "for row in df.itertuples():\n",
    "    user_item_matrix[row.userId - 1, row.cat_movie - 1] = row.rating\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "user_item_matrix = torch.tensor(user_item_matrix, dtype=torch.float32)\n",
    "\n",
    "# Split into train and test (80-20 split)\n",
    "train_matrix = user_item_matrix.clone()\n",
    "test_matrix = user_item_matrix.clone()\n",
    "test_matrix[np.random.rand(*test_matrix.shape) < 0.8] = 0  # Mask 80% of interactions for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAERecommender(nn.Module):\n",
    "    def __init__(self, num_items, latent_dim=50):\n",
    "        super(VAERecommender, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_items, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu = nn.Linear(100, latent_dim)\n",
    "        self.logvar = nn.Linear(100, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, num_items),\n",
    "            nn.Sigmoid()  # Output probabilities of items\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        mu, logvar = self.mu(encoded), self.logvar(encoded)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std  # Reparameterization trick\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(reconstructed, original, mu, logvar):\n",
    "    reconstruction_loss = nn.MSELoss()(reconstructed, original)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + 0.0001 * kl_divergence  # Small weight on KL term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.4308\n",
      "Epoch 2/30, Loss: 0.4269\n",
      "Epoch 3/30, Loss: 0.4188\n",
      "Epoch 4/30, Loss: 0.4104\n",
      "Epoch 5/30, Loss: 0.4022\n",
      "Epoch 6/30, Loss: 0.3938\n",
      "Epoch 7/30, Loss: 0.3843\n",
      "Epoch 8/30, Loss: 0.3733\n",
      "Epoch 9/30, Loss: 0.3605\n",
      "Epoch 10/30, Loss: 0.3484\n",
      "Epoch 11/30, Loss: 0.3337\n",
      "Epoch 12/30, Loss: 0.3185\n",
      "Epoch 13/30, Loss: 0.3033\n",
      "Epoch 14/30, Loss: 0.2879\n",
      "Epoch 15/30, Loss: 0.2740\n",
      "Epoch 16/30, Loss: 0.2595\n",
      "Epoch 17/30, Loss: 0.2468\n",
      "Epoch 18/30, Loss: 0.2375\n",
      "Epoch 19/30, Loss: 0.2290\n",
      "Epoch 20/30, Loss: 0.2227\n",
      "Epoch 21/30, Loss: 0.2175\n",
      "Epoch 22/30, Loss: 0.2150\n",
      "Epoch 23/30, Loss: 0.2123\n",
      "Epoch 24/30, Loss: 0.2106\n",
      "Epoch 25/30, Loss: 0.2097\n",
      "Epoch 26/30, Loss: 0.2089\n",
      "Epoch 27/30, Loss: 0.2083\n",
      "Epoch 28/30, Loss: 0.2080\n",
      "Epoch 29/30, Loss: 0.2078\n",
      "Epoch 30/30, Loss: 0.2075\n"
     ]
    }
   ],
   "source": [
    "# Define model, optimizer\n",
    "latent_dim = 50\n",
    "model = VAERecommender(num_items, latent_dim=latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_matrix = train_matrix.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reconstructed, mu, logvar = model(train_matrix)\n",
    "    loss = vae_loss(reconstructed, train_matrix, mu, logvar)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movies for user 10: [[8899 8029 7159 ...  313  896 2143]]\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_id, model, num_recommendations=5):\n",
    "    model.eval()\n",
    "    user_vector = train_matrix[user_id].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ratings, _, _ = model(user_vector)\n",
    "\n",
    "    recommended_movies = predicted_ratings.cpu().numpy().argsort()[::-1][:num_recommendations]\n",
    "    return recommended_movies\n",
    "\n",
    "# Example recommendation\n",
    "user_id = 10  # Example user\n",
    "recommended_movies = recommend_for_user(user_id, model)\n",
    "print(f\"Recommended movies for user {user_id}: {recommended_movies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset (MovieLens sample)\n",
    "df = pd.read_csv(\"./ml-latest-small/ratings.csv\")\n",
    "\n",
    "# Encode users and items as integers\n",
    "df['userId'] = df['userId'].astype(\"category\").cat.codes\n",
    "df['movieId'] = df['movieId'].astype(\"category\").cat.codes\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['userId'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['movieId'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = MovieLensDataset(train_data)\n",
    "test_dataset = MovieLensDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output rating prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embedded = self.user_embedding(user_ids)\n",
    "        item_embedded = self.item_embedding(item_ids)\n",
    "        x = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Get number of unique users and items\n",
    "num_users = df['userId'].nunique()\n",
    "num_items = df['movieId'].nunique()\n",
    "\n",
    "# Instantiate model\n",
    "model = NCF(num_users, num_items, embedding_dim=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0693\n",
      "Epoch 2/10, Loss: 0.8170\n",
      "Epoch 3/10, Loss: 0.7517\n",
      "Epoch 4/10, Loss: 0.7078\n",
      "Epoch 5/10, Loss: 0.6709\n",
      "Epoch 6/10, Loss: 0.6381\n",
      "Epoch 7/10, Loss: 0.6075\n",
      "Epoch 8/10, Loss: 0.5756\n",
      "Epoch 9/10, Loss: 0.5433\n",
      "Epoch 10/10, Loss: 0.5105\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for users, items, ratings in train_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items)\n",
    "        loss = loss_fn(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for users, items, ratings in test_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "        predictions = model(users, items)\n",
    "        \n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_actuals.extend(ratings.cpu().numpy())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(all_actuals, all_preds))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 10: [5875 2409 7650 2645 2934]\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_id, model, num_recommendations=5):\n",
    "    model.eval()\n",
    "    user_tensor = torch.tensor([user_id] * num_items, dtype=torch.long).to(device)\n",
    "    item_tensor = torch.arange(num_items, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(user_tensor, item_tensor)\n",
    "\n",
    "    top_items = scores.cpu().numpy().argsort()[-num_recommendations:][::-1]\n",
    "    return top_items\n",
    "\n",
    "user_id = 10  # Example user ID\n",
    "recommended_items = recommend_for_user(user_id, model)\n",
    "print(f\"Recommended items for user {user_id}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Linear(embedding_dim * 2, 1)  # Computes attention scores\n",
    "\n",
    "    def forward(self, user_emb, item_emb):\n",
    "        combined = torch.cat([user_emb, item_emb], dim=-1)  # Concatenate embeddings\n",
    "        attn_weights = torch.sigmoid(self.attention(combined))  # Compute attention scores\n",
    "        weighted_emb = attn_weights * combined  # Apply attention\n",
    "        return weighted_emb\n",
    "\n",
    "class NCFWithAttention(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50):\n",
    "        super(NCFWithAttention, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        self.attention = AttentionLayer(embedding_dim)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output rating prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embedded = self.user_embedding(user_ids)\n",
    "        item_embedded = self.item_embedding(item_ids)\n",
    "\n",
    "        weighted_features = self.attention(user_embedded, item_embedded)  # Apply attention\n",
    "        output = self.fc_layers(weighted_features)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1970\n",
      "Epoch 2/10, Loss: 0.8338\n",
      "Epoch 3/10, Loss: 0.7651\n",
      "Epoch 4/10, Loss: 0.7199\n",
      "Epoch 5/10, Loss: 0.6833\n",
      "Epoch 6/10, Loss: 0.6522\n",
      "Epoch 7/10, Loss: 0.6239\n",
      "Epoch 8/10, Loss: 0.5963\n",
      "Epoch 9/10, Loss: 0.5695\n",
      "Epoch 10/10, Loss: 0.5396\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "num_users = df['userId'].nunique()\n",
    "num_items = df['movieId'].nunique()\n",
    "model = NCFWithAttention(num_users, num_items, embedding_dim=50)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for users, items, ratings in train_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items)\n",
    "        loss = loss_fn(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for users, items, ratings in test_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "        predictions = model(users, items)\n",
    "        \n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_actuals.extend(ratings.cpu().numpy())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(all_actuals, all_preds))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 10: [5862 6403 6298 6680 9004]\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_id, model, num_recommendations=5):\n",
    "    model.eval()\n",
    "    user_tensor = torch.tensor([user_id] * num_items, dtype=torch.long).to(device)\n",
    "    item_tensor = torch.arange(num_items, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(user_tensor, item_tensor)\n",
    "\n",
    "    top_items = scores.cpu().numpy().argsort()[-num_recommendations:][::-1]\n",
    "    return top_items\n",
    "\n",
    "user_id = 10  # Example user ID\n",
    "recommended_items = recommend_for_user(user_id, model)\n",
    "print(f\"Recommended items for user {user_id}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads=2):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, user_emb, item_emb):\n",
    "        combined = torch.cat([user_emb.unsqueeze(1), item_emb.unsqueeze(1)], dim=1)  # Shape: (batch, 2, embedding_dim)\n",
    "        attn_output, _ = self.attention(combined, combined, combined)  # Self-attention\n",
    "        return attn_output.mean(dim=1)  # Aggregate attention outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50, num_heads=2):\n",
    "        super(TransformerRecommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        self.self_attention = SelfAttentionLayer(embedding_dim, num_heads)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output rating prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embedded = self.user_embedding(user_ids)\n",
    "        item_embedded = self.item_embedding(item_ids)\n",
    "\n",
    "        attended_features = self.self_attention(user_embedded, item_embedded)  # Self-Attention\n",
    "        output = self.fc_layers(attended_features)\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.2665\n",
      "Epoch 2/10, Loss: 0.8947\n",
      "Epoch 3/10, Loss: 0.8208\n",
      "Epoch 4/10, Loss: 0.7602\n",
      "Epoch 5/10, Loss: 0.7080\n",
      "Epoch 6/10, Loss: 0.6609\n",
      "Epoch 7/10, Loss: 0.6216\n",
      "Epoch 8/10, Loss: 0.5873\n",
      "Epoch 9/10, Loss: 0.5540\n",
      "Epoch 10/10, Loss: 0.5238\n"
     ]
    }
   ],
   "source": [
    "# Get the number of unique users and items\n",
    "num_users = df['userId'].nunique()\n",
    "num_items = df['movieId'].nunique()\n",
    "\n",
    "# Define model\n",
    "model = TransformerRecommender(num_users, num_items, embedding_dim=50, num_heads=2)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for users, items, ratings in train_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items)\n",
    "        loss = loss_fn(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for users, items, ratings in test_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "        predictions = model(users, items)\n",
    "        \n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_actuals.extend(ratings.cpu().numpy())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(all_actuals, all_preds))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 10: [7435  924  405 7683 1648]\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_id, model, num_recommendations=5):\n",
    "    model.eval()\n",
    "    user_tensor = torch.tensor([user_id] * num_items, dtype=torch.long).to(device)\n",
    "    item_tensor = torch.arange(num_items, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(user_tensor, item_tensor)\n",
    "\n",
    "    top_items = scores.cpu().numpy().argsort()[-num_recommendations:][::-1]\n",
    "    return top_items\n",
    "\n",
    "user_id = 10  # Example user ID\n",
    "recommended_items = recommend_for_user(user_id, model)\n",
    "print(f\"Recommended items for user {user_id}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT and positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)  # Add positional encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50, num_heads=2, num_transformer_layers=2):\n",
    "        super(BERTRecommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output rating prediction\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_emb = self.user_embedding(user_ids).unsqueeze(1)  # Shape: (batch, 1, embedding_dim)\n",
    "        item_emb = self.item_embedding(item_ids).unsqueeze(1)  # Shape: (batch, 1, embedding_dim)\n",
    "\n",
    "        combined_emb = torch.cat([user_emb, item_emb], dim=1)  # Shape: (batch, 2, embedding_dim)\n",
    "        combined_emb = self.positional_encoding(combined_emb)\n",
    "\n",
    "        transformer_output = self.transformer_encoder(combined_emb)  # Shape: (batch, 2, embedding_dim)\n",
    "        attended_features = transformer_output.mean(dim=1)  # Aggregate across positions\n",
    "\n",
    "        return self.fc_layers(attended_features).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baba\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0799\n",
      "Epoch 2/10, Loss: 0.8351\n",
      "Epoch 3/10, Loss: 0.7791\n",
      "Epoch 4/10, Loss: 0.7457\n",
      "Epoch 5/10, Loss: 0.7221\n",
      "Epoch 6/10, Loss: 0.7069\n",
      "Epoch 7/10, Loss: 0.6949\n",
      "Epoch 8/10, Loss: 0.6851\n",
      "Epoch 9/10, Loss: 0.6748\n",
      "Epoch 10/10, Loss: 0.6680\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "num_users = df['userId'].nunique()\n",
    "num_items = df['movieId'].nunique()\n",
    "model = BERTRecommender(num_users, num_items, embedding_dim=50, num_heads=2, num_transformer_layers=2)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for users, items, ratings in train_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items)\n",
    "        loss = loss_fn(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_actuals = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for users, items, ratings in test_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "        predictions = model(users, items)\n",
    "        \n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_actuals.extend(ratings.cpu().numpy())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(all_actuals, all_preds))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended items for user 10: [ 602  659 4900 7742 7793]\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(user_id, model, num_recommendations=5):\n",
    "    model.eval()\n",
    "    user_tensor = torch.tensor([user_id] * num_items, dtype=torch.long).to(device)\n",
    "    item_tensor = torch.arange(num_items, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(user_tensor, item_tensor)\n",
    "\n",
    "    top_items = scores.cpu().numpy().argsort()[-num_recommendations:][::-1]\n",
    "    return top_items\n",
    "\n",
    "user_id = 10  # Example user ID\n",
    "recommended_items = recommend_for_user(user_id, model)\n",
    "print(f\"Recommended items for user {user_id}: {recommended_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
